{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging Student Information to Enhance College Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Development and Validation Results Using Ensemble Algorithms\n",
    "- Random Forests\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "import scikitplot as skplt\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "df = pd.read_csv('data/DataForHSStudentsFor2YearCollege.csv')\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Develop models using ensemble algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train/test datasets. Use the default test_size of 25%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = ['persistIndicator']),df['persistIndicator'],random_state = 42, stratify = df['persistIndicator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale all the variables\n",
    "transformer = make_column_transformer((StandardScaler(), list(X_train.columns)),\n",
    "                                      remainder = 'passthrough' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Build a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a pipeline for a random forest model\n",
    "randomForestPipe = Pipeline([('transformer',transformer),\n",
    "                     ('randomForest',RandomForestClassifier(oob_score=True, random_state = 42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune the random forest - define parameters for the search algorithm\n",
    "n_estimators = np.linspace(100, 1000, int((1000-100)/200) + 1, dtype=int) # number of trees\n",
    "max_features = ['auto','log2', None] # number of features to consider at every split\n",
    "max_depth = [1, 5, 10, 20, 50, 75, 100, 150, 200] # maximum number of levels in tree\n",
    "min_samples_leaf = [10, 20, 50] # minimum number of samples required at each leaf\n",
    "bootstrap = [True, False] # method of selecting samples for training each tree\n",
    "criterion = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "random_grid = {'randomForest__n_estimators':n_estimators,\n",
    "               'randomForest__max_features':max_features,\n",
    "               'randomForest__max_depth':max_depth,\n",
    "               'randomForest__min_samples_leaf':min_samples_leaf,\n",
    "               'randomForest__bootstrap':bootstrap,\n",
    "               'randomForest__criterion':criterion}\n",
    "\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search for the random forest\n",
    "roc_grid_randomForest = RandomizedSearchCV(randomForestPipe,\n",
    "                        param_distributions = random_grid, \n",
    "                        random_state = 42, n_jobs = 4).fit(X_train, y_train)\n",
    "roc_grid_randomForest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a random forest with the best parameters\n",
    "randomForestBestPipe = Pipeline([('transformer',transformer),\n",
    "                     ('randomForest',RandomForestClassifier(\n",
    "                     n_estimators = roc_grid_randomForest.best_params_.get('randomForest__n_estimators'), \n",
    "                     min_samples_leaf = roc_grid_randomForest.best_params_.get('randomForest__min_samples_leaf'),\n",
    "                     max_features = roc_grid_randomForest.best_params_.get('randomForest__max_features'),\n",
    "                     max_depth = roc_grid_randomForest.best_params_.get('randomForest__max_depth'),\n",
    "                     bootstrap = roc_grid_randomForest.best_params_.get('randomForest__bootstrap'),\n",
    "                     criterion = roc_grid_randomForest.best_params_.get('randomForest__criterion'),\n",
    "                     random_state = 42))]).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate predicted values, both the classifier and the probability\n",
    "y_train_predicted = randomForestBestPipe.predict(X_train)\n",
    "y_test_predicted = randomForestBestPipe.predict(X_test)\n",
    "\n",
    "y_train_predicted_proba = randomForestBestPipe.predict_proba(X_train)\n",
    "y_test_predicted_proba = randomForestBestPipe.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the confusion matrix\n",
    "dataList = [y_train,y_test]\n",
    "predictedList = [y_train_predicted, y_test_predicted]\n",
    "dataLabels = ['Train Sample','Test Sample']\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,15))\n",
    "#plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "for i,j in enumerate(predictedList):\n",
    "    confusionMatrix= confusion_matrix(dataList[i],j)\n",
    "    disp = ConfusionMatrixDisplay(confusionMatrix, display_labels = ['Persisted','Dropout'])\n",
    "    disp.plot(ax=ax[i], xticks_rotation=45)\n",
    "    disp.ax_.set_title(dataLabels[i])\n",
    "    disp.im_.colorbar.remove()\n",
    "    if i!=0:\n",
    "        disp.ax_.set_ylabel('')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the train sample, there are 281 instances of false negatives (classified as persistent even though a dropout), or missed opportunities where the necessary interventions were not taken to retain students in college. The test sample reports 88 false negatives.\n",
    "- The train sample also shows a high occurrence of false positives, or instances where the investment was misallocated, with 50 cases. This statistic decreases to 21 in the test sample.\n",
    "- It is imperative to strive for minimizing both false negatives and false positives in order to maximize success in retaining students and effectively utilizing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate summary stats, accuracy, precision, recall, f1-score and AUC\n",
    "def generateModelSummaryStats(y_train, y_train_predicted, y_test, y_test_predicted,y_train_predicted_proba,y_test_predicted_proba):\n",
    "    trainStats = []\n",
    "    testStats = []\n",
    "    \n",
    "    trainStats.append(np.round(accuracy_score(y_train,y_train_predicted),3))\n",
    "    if (np.sum(y_train_predicted) == 0):\n",
    "        trainStats.append(\"N/A\")\n",
    "    else:\n",
    "        trainStats.append(np.round(precision_score(y_train,y_train_predicted),3))\n",
    "    trainStats.append(np.round(recall_score(y_train,y_train_predicted),3))\n",
    "    trainStats.append(np.round(f1_score(y_train,y_train_predicted),3))\n",
    "    trainStats.append(np.round(roc_auc_score(y_train, y_train_predicted_proba[:,1]),3))\n",
    "    \n",
    "    testStats.append(np.round(accuracy_score(y_test,y_test_predicted),3))\n",
    "    if (np.sum(y_test_predicted) == 0):\n",
    "        testStats.append(\"N/A\")\n",
    "    else:\n",
    "        testStats.append(np.round(precision_score(y_test,y_test_predicted),3))\n",
    "    testStats.append(np.round(recall_score(y_test,y_test_predicted),3))\n",
    "    testStats.append(np.round(f1_score(y_test,y_test_predicted),3))\n",
    "    testStats.append(np.round(roc_auc_score(y_test, y_test_predicted_proba[:,1]),3))\n",
    "    \n",
    "    #Summarize results\n",
    "    listOfStats = ['Accuracy','Precision','Recall','F1 Score','AUC']\n",
    "    dfStats = pd.DataFrame(zip(listOfStats, trainStats, testStats), columns = ['Metric','Train Sample','Test Sample'])\n",
    "    return(dfStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate summary statistics\n",
    "generateModelSummaryStats(y_train, y_train_predicted, y_test, y_test_predicted,y_train_predicted_proba,y_test_predicted_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC for the Logistic Regression model with default parameters\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "RocCurveDisplay.from_estimator(randomForestBestPipe, X_train, y_train,ax=ax,label = \"Logistic Regression - Train\")\n",
    "RocCurveDisplay.from_estimator(randomForestBestPipe, X_test, y_test,ax=ax,label = \"Logistic Regression - Test\")\n",
    "plt.grid()\n",
    "plt.plot(np.arange(0, 1.1, .1), np.arange(0, 1.1, .1), label = 'baseline');\n",
    "plt.xlabel('False Positive (Persisted) Rate')\n",
    "plt.ylabel('True Positive (Dropout) Rate')\n",
    "plt.title('ROC for True Positives (= Dropout)')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our model's ability to identify students who dropped out can be analyzed through the above curve.\n",
    "- Specifically, on the test sample, it predicts 40% of the students who dropped out within the bottom 20% of students who persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame(data={\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': randomForestBestPipe.named_steps['randomForest'].feature_importances_,\n",
    "    'Importance (Abs)':abs(randomForestBestPipe.named_steps['randomForest'].feature_importances_)\n",
    "})\n",
    "\n",
    "importances = importances.sort_values(by='Importance (Abs)', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize = (30, 15))\n",
    "ax.bar(x=importances['Feature'], height=importances['Importance (Abs)'])\n",
    "ax.set_title('Feature Importances', size = 30)\n",
    "ax.set_ylabel('Coefficient (Abs)', fontsize = 30)\n",
    "ax.tick_params(axis = 'x',labelrotation=45, labelsize=15)\n",
    "ax.tick_params(axis = 'y',  labelsize=30)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of a student persisting to their second year of college is most strongly influenced by the following top five factors:\n",
    "\n",
    "- The rating of their high school\n",
    "- Participitation in college classes during the summer before they start college  Participation in a Success program with a scholarship\n",
    "- Participation in the Success program\n",
    "- The area where they went for college\n",
    "- The area they grew up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a table of the observed drop-out rate for each decile calculated based on the predicted drop-out rate\n",
    "y_data = [y_train, y_test]\n",
    "y_predict_proba = [y_train_predicted_proba, y_test_predicted_proba]\n",
    "\n",
    "for i in range(0,2):\n",
    "    y_Joined = pd.DataFrame(y_data[i]).reset_index(drop=True).join(pd.DataFrame(y_predict_proba[i]).reset_index(drop=True)).rename(columns = {1:'dropOutProb'})\n",
    "    label = list(range(1,11,1))\n",
    "    y_Joined['Decile'] = pd.qcut(y_Joined['dropOutProb'],10, label )\n",
    "\n",
    "    #Count of drop-out students in each bin\n",
    "    dfSubscribed = pd.DataFrame(y_Joined.groupby('Decile')['persistIndicator'].sum()).reset_index().rename(columns = {\"persistIndicator\":\"Dropout\"})\n",
    "\n",
    "    #Count of records in each bin\n",
    "    dfTotal = pd.DataFrame(y_Joined.groupby('Decile')['persistIndicator'].count()).reset_index().rename(columns = {\"persistIndicator\":\"Total\"})\n",
    "\n",
    "    #Generate a dataset with the drop-out rate for each decile\n",
    "    dfStats = pd.merge(dfSubscribed, dfTotal, on = 'Decile')\n",
    "\n",
    "    dfStats['Observed Dropout Rate'] = dfStats['Dropout'] / dfStats['Total']\n",
    "    if i == 0:\n",
    "        dfStatsTrain = dfStats.copy()\n",
    "    else:\n",
    "        dfStatsTest = dfStats.copy()\n",
    "        \n",
    "dfStatsTrain\n",
    "\n",
    "dfStatsTrain.to_csv(\"data/DecileStats_RandomForest_Train.csv\")\n",
    "dfStatsTest.to_csv(\"data/DecileStats_RandomForest_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a chart to observe the rank ordering capability of the model\n",
    "fig,ax = plt.subplots(figsize = (20, 7))\n",
    "\n",
    "X_label = np.arange(1,11,1)\n",
    "plt.bar(X_label - 0.2,  dfStatsTrain['Observed Dropout Rate'],width = 0.4,label = 'Train Dataset')\n",
    "plt.bar(X_label + 0.2,  dfStatsTest['Observed Dropout Rate'], width = 0.4,label = 'Test Dataset' )\n",
    "plt.xlabel(\"Decile based on the Predicted Dropout Rate\")\n",
    "plt.ylabel(\"Observed Drop-out Rate\")\n",
    "plt.title(\"Observed Dropout Rate by Decile for the Random Forest Model\")\n",
    "plt.xticks(X_label)\n",
    "plt.yticks(np.linspace(0,1,11))\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart depicts a noteworthy improvement in the model's capacity to rank order after the optimization of its hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Build a gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a pipeline for a gradient boosting model\n",
    "gradientBoostingPipe = Pipeline([('transformer',transformer),\n",
    "                     ('gradientBoosting',GradientBoostingClassifier(random_state = 42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune the gradient boost - define parameters for the search algorithm\n",
    "n_estimators = np.linspace(100, 1000, int((1000-100)/200) + 1, dtype=int) # number of boosting stages\n",
    "max_features = ['auto','log2', None] # number of features to consider at every split\n",
    "max_depth = [1, 5, 10, 20, 50, 75, 100, 150, 200] # Maximum depth of the individual regression estimators\n",
    "min_samples_leaf = [10, 20, 50] # minimum number of samples required at each leaf\n",
    "criterion = ['friedman_mse', 'squared_error']\n",
    "\n",
    "random_grid = {'gradientBoosting__n_estimators':n_estimators,\n",
    "               'gradientBoosting__max_features':max_features,\n",
    "               'gradientBoosting__max_depth':max_depth,\n",
    "               'gradientBoosting__min_samples_leaf':min_samples_leaf,\n",
    "               'gradientBoosting__criterion':criterion}\n",
    "\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search for the gradient boosting classifier\n",
    "roc_grid_gradientBoosting = RandomizedSearchCV(gradientBoostingPipe,\n",
    "                        param_distributions = random_grid, \n",
    "                        random_state = 42, n_jobs = 4).fit(X_train, y_train)\n",
    "roc_grid_gradientBoosting.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a gradient boosting classifierwith the best parameters\n",
    "gradientBoostingBestPipe = Pipeline([('transformer',transformer),\n",
    "                     ('gradientBoosting',GradientBoostingClassifier(\n",
    "                     n_estimators = roc_grid_gradientBoosting.best_params_.get('gradientBoosting__n_estimators'), \n",
    "                     min_samples_leaf = roc_grid_gradientBoosting.best_params_.get('gradientBoosting__min_samples_leaf'),\n",
    "                     max_features = roc_grid_gradientBoosting.best_params_.get('gradientBoosting__max_features'),\n",
    "                     max_depth = roc_grid_gradientBoosting.best_params_.get('gradientBoosting__max_depth'),\n",
    "                     criterion = roc_grid_gradientBoosting.best_params_.get('gradientBoosting__criterion'),\n",
    "                     random_state = 42))]).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate predicted values, both the classifier and the probability\n",
    "y_train_predicted = gradientBoostingBestPipe.predict(X_train)\n",
    "y_test_predicted = gradientBoostingBestPipe.predict(X_test)\n",
    "\n",
    "y_train_predicted_proba = gradientBoostingBestPipe.predict_proba(X_train)\n",
    "y_test_predicted_proba = gradientBoostingBestPipe.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the confusion matrix\n",
    "dataList = [y_train,y_test]\n",
    "predictedList = [y_train_predicted, y_test_predicted]\n",
    "dataLabels = ['Train Sample','Test Sample']\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,15))\n",
    "#plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "for i,j in enumerate(predictedList):\n",
    "    confusionMatrix= confusion_matrix(dataList[i],j)\n",
    "    disp = ConfusionMatrixDisplay(confusionMatrix, display_labels = ['Persisted','Dropout'])\n",
    "    disp.plot(ax=ax[i], xticks_rotation=45)\n",
    "    disp.ax_.set_title(dataLabels[i])\n",
    "    disp.im_.colorbar.remove()\n",
    "    if i!=0:\n",
    "        disp.ax_.set_ylabel('')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the train sample, there are 281 instances of false negatives (classified as persistent even though a dropout), or missed opportunities where the necessary interventions were not taken to retain students in college. The test sample reports 88 false negatives.\n",
    "- The train sample also shows a high occurrence of false positives, or instances where the investment was misallocated, with 50 cases. This statistic decreases to 21 in the test sample.\n",
    "- It is imperative to strive for minimizing both false negatives and false positives in order to maximize success in retaining students and effectively utilizing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate summary stats, accuracy, precision, recall, f1-score and AUC\n",
    "def generateModelSummaryStats(y_train, y_train_predicted, y_test, y_test_predicted,y_train_predicted_proba,y_test_predicted_proba):\n",
    "    trainStats = []\n",
    "    testStats = []\n",
    "    \n",
    "    trainStats.append(np.round(accuracy_score(y_train,y_train_predicted),3))\n",
    "    if (np.sum(y_train_predicted) == 0):\n",
    "        trainStats.append(\"N/A\")\n",
    "    else:\n",
    "        trainStats.append(np.round(precision_score(y_train,y_train_predicted),3))\n",
    "    trainStats.append(np.round(recall_score(y_train,y_train_predicted),3))\n",
    "    trainStats.append(np.round(f1_score(y_train,y_train_predicted),3))\n",
    "    trainStats.append(np.round(roc_auc_score(y_train, y_train_predicted_proba[:,1]),3))\n",
    "    \n",
    "    testStats.append(np.round(accuracy_score(y_test,y_test_predicted),3))\n",
    "    if (np.sum(y_test_predicted) == 0):\n",
    "        testStats.append(\"N/A\")\n",
    "    else:\n",
    "        testStats.append(np.round(precision_score(y_test,y_test_predicted),3))\n",
    "    testStats.append(np.round(recall_score(y_test,y_test_predicted),3))\n",
    "    testStats.append(np.round(f1_score(y_test,y_test_predicted),3))\n",
    "    testStats.append(np.round(roc_auc_score(y_test, y_test_predicted_proba[:,1]),3))\n",
    "    \n",
    "    #Summarize results\n",
    "    listOfStats = ['Accuracy','Precision','Recall','F1 Score','AUC']\n",
    "    dfStats = pd.DataFrame(zip(listOfStats, trainStats, testStats), columns = ['Metric','Train Sample','Test Sample'])\n",
    "    return(dfStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate summary statistics\n",
    "generateModelSummaryStats(y_train, y_train_predicted, y_test, y_test_predicted,y_train_predicted_proba,y_test_predicted_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC for the Logistic Regression model with default parameters\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "RocCurveDisplay.from_estimator(gradientBoostingBestPipe, X_train, y_train,ax=ax,label = \"Gradient Boosting - Train\")\n",
    "RocCurveDisplay.from_estimator(gradientBoostingBestPipe, X_test, y_test,ax=ax,label = \"Gradient Boosting - Test\")\n",
    "plt.grid()\n",
    "plt.plot(np.arange(0, 1.1, .1), np.arange(0, 1.1, .1), label = 'baseline');\n",
    "plt.xlabel('False Positive (Persisted) Rate')\n",
    "plt.ylabel('True Positive (Dropout) Rate')\n",
    "plt.title('ROC for True Positives (= Dropout)')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame(data={\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': gradientBoostingBestPipe.named_steps['gradientBoosting'].feature_importances_,\n",
    "    'Importance (Abs)':abs(gradientBoostingBestPipe.named_steps['gradientBoosting'].feature_importances_)\n",
    "})\n",
    "\n",
    "importances = importances.sort_values(by='Importance (Abs)', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize = (30, 15))\n",
    "ax.bar(x=importances['Feature'], height=importances['Importance (Abs)'])\n",
    "ax.set_title('Feature Importances', size = 30)\n",
    "ax.set_ylabel('Coefficient (Abs)', fontsize = 30)\n",
    "ax.tick_params(axis = 'x',labelrotation=45, labelsize=15)\n",
    "ax.tick_params(axis = 'y',  labelsize=30)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a table of the observed drop-out rate for each decile calculated based on the predicted drop-out rate\n",
    "y_data = [y_train, y_test]\n",
    "y_predict_proba = [y_train_predicted_proba, y_test_predicted_proba]\n",
    "\n",
    "for i in range(0,2):\n",
    "    y_Joined = pd.DataFrame(y_data[i]).reset_index(drop=True).join(pd.DataFrame(y_predict_proba[i]).reset_index(drop=True)).rename(columns = {1:'dropOutProb'})\n",
    "    label = list(range(1,11,1))\n",
    "    y_Joined['Decile'] = pd.qcut(y_Joined['dropOutProb'],10, label )\n",
    "\n",
    "    #Count of drop-out students in each bin\n",
    "    dfSubscribed = pd.DataFrame(y_Joined.groupby('Decile')['persistIndicator'].sum()).reset_index().rename(columns = {\"persistIndicator\":\"Dropout\"})\n",
    "\n",
    "    #Count of records in each bin\n",
    "    dfTotal = pd.DataFrame(y_Joined.groupby('Decile')['persistIndicator'].count()).reset_index().rename(columns = {\"persistIndicator\":\"Total\"})\n",
    "\n",
    "    #Generate a dataset with the drop-out rate for each decile\n",
    "    dfStats = pd.merge(dfSubscribed, dfTotal, on = 'Decile')\n",
    "\n",
    "    dfStats['Observed Dropout Rate'] = dfStats['Dropout'] / dfStats['Total']\n",
    "    if i == 0:\n",
    "        dfStatsTrain = dfStats.copy()\n",
    "    else:\n",
    "        dfStatsTest = dfStats.copy()\n",
    "        \n",
    "dfStatsTrain\n",
    "\n",
    "dfStatsTrain.to_csv(\"data/DecileStats_GradientBoosting_Train.csv\")\n",
    "dfStatsTest.to_csv(\"data/DecileStats_GradientBoosting_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a chart to observe the rank ordering capability of the model\n",
    "fig,ax = plt.subplots(figsize = (20, 7))\n",
    "\n",
    "X_label = np.arange(1,11,1)\n",
    "plt.bar(X_label - 0.2,  dfStatsTrain['Observed Dropout Rate'],width = 0.4,label = 'Train Dataset')\n",
    "plt.bar(X_label + 0.2,  dfStatsTest['Observed Dropout Rate'], width = 0.4,label = 'Test Dataset' )\n",
    "plt.xlabel(\"Decile based on the Predicted Dropout Rate\")\n",
    "plt.ylabel(\"Observed Drop-out Rate\")\n",
    "plt.title(\"Observed Dropout Rate by Decile for the Gradient Boosting Model\")\n",
    "plt.xticks(X_label)\n",
    "plt.yticks(np.linspace(0,1,11))\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
